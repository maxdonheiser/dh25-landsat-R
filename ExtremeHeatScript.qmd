---
title: "Satellite Imagery and Extreme Heat"
author: "Max Donheiser"
format: html
editor: visual
---

## Satellite Imagery and Extreme Heat

First let's load the packages we need:

```{r}

library(dplyr) # for manipulating data
library(ggplot2) # for visualization
library(terra) # for working with raster and vector data
```

```{undefined}
#| echo: true

```

Now we need some data. Conveniently, the `terra` package in R has great naming conventions for loading data. It’s also really easy to plot geodata to preview how it looks. Let’s start with a landsat image.

```{r}
#| echo: true
landsat <- rast('data/LC09_L2SP_196023_20240501_20240502_02_T1_ST_B10.TIF')

plot(landsat)
```

Now we can also load the vector data for Hamburg which contains the geographical boundaries for each of the city’s districts. We can do this by directly calling an API endpoint from Hamburg’s (Office for Geoinformation and Surveying)\[<https://www.hamburg.de/politik-und-verwaltung/behoerden/behoerde-fuer-stadtentwicklung-und-wohnen/aemter-und-landesbetrieb/landesbetrieb-geoinformation-und-vermessung>\]. In this example, the vector data we’re working with is in GeoJSON format. However, the `vect` function in `terra` can also handle other vector data formats, such as Shapefiles.

```{r}
hamburg <- vect('https://api.hamburg.de/datasets/v1/verwaltungsgrenzen/collections/stadtteile/items?limit=104&f=json')

plot(hamburg)
```

Normally you might want to click on and explore your dataset from the R environment the same way you could with a normal data frame or the `sf` package. But in terra you can't.

But you can preview your data using the standard `head` function, which gives a nice overview of what data you’re working with.

```{r}
#| echo: true
hamburg |>
  head()
```

Here we have information on the city district (`bezirk`) and the neighborhood (`stadtteil`).

We can also filter the data, although it functions a bit differently than with normal dataframes.

Let’s try just plotting the neighborhoods in Hamburg’s Altona district.

```{r}
#| echo: true
altona <- subset(hamburg, hamburg$bezirk_name=="Altona")

plot(altona)
```

If you wanted to stick to the classic `tidyr` and `dplyr` syntax, then you could use the helpful `tidyterra` package. It allows us to interact with spatial raster or vector data just as we would with a dataframe.

```{r}
#| include: false
library(tidyterra)
```

Let's filter out the islands of Hamburg and focus on the city

```{r}
#| echo: true

hamburg <- hamburg |>
  filter(!(stadtteil_name %in% c("Neuwerk", "Nigehörn", "Scharhörn")))

plot(hamburg)
```

## **Dealing with projections**

You may have noticed that the x and y axes for our Landsat image and Hamburg are quite different. That’s because the they are using two different coordinate systems. It also means that if we tried to plot both together, it wouldn’t really work out.

In technical terms, that means the data sets have different coordinate reference systems, also known as CRS. We can check which CRS is being used quite easily:

```{r}
#| echo: true
crs(landsat)

crs(hamburg)


```

That returns a bunch of information, some of which we don't need.

An easier alternative?

```{r}
#| echo: true
same.crs(landsat, hamburg)
```

Great, a simple no. In order to fix this, we need to reproject either Hamburg or the Landsat data.

In this case, let's reproject the data for Hamburg since it’s not as detailed as the Landsat data and will go a bit faster.

```{r}
new_crs <-  crs(landsat)

hamburg <-  hamburg |>
  project(new_crs)

plot(hamburg)
```

Wonderful, this seems to be a bit closer to what we want. To double check that everything worked, we can plot the Hamburg districts on top of the raster data.

```{r}
plot(landsat)
plot(hamburg, add=TRUE)
```

## **Cropping and masking**

Now, you may have noticed that the Landsat image is way bigger than we need.

We can take two measures to reduce the amount of data we’re working with, and so speed up calculations that we’ll do later.

First we can crop the image. This is the same thing as cropping out that random stranger who happened to be in your holiday vacation photos.

```{r}
#| echo: true
landsat <- crop(landsat, hamburg)
plot(landsat)
plot(hamburg, add=TRUE)
```

Great, now we’re focused just on the region of Hamburg.

But we can also take it a step further and mask the Landsat image.

This basically means removing parts of the image that are not within our zone of interest. Think of it as you would a PNG file that has a transparent background in parts of it.

```{r}
landsat <- mask(landsat, hamburg)
plot(landsat)
plot(hamburg, add=TRUE)
```

You can speed things up by calling the `crop` function and setting the variable `mask` to `TRUE`.

## **Bands and calculations with raster data**

Another thing you may have noticed is that the scale for the Landsat image doesn’t make sense. We’re working with surface temperature, and those numbers surely don’t represent anything close to Celsius or Fahrenheit. That’s because the data is natively in a metric called digital numbers (DN).

Unfortunately, I can’t really tell you what this means. Fortunately, Landsat provides us with a nice little [scaling formula](https://www.usgs.gov/landsat-missions/landsat-collection-2-level-2-science-products) to turn these values into Kelvins: K = (DN \* 0.00341802) + 149.0

In raster data work is that, much like mutating an entire column, each pixel will be put through the given formula. Instead of columns, however, we have something called bands. Let’s see what different bands we have.

```{r}
varnames(landsat)
```

Alright, let’s make a new band for the same data in Kelvin.

```{r}
landsat$lst_kelvin <-  (landsat$LC09_L2SP_196023_20240501_20240502_02_T1_ST_B10 * 0.00341802) + 149.0

plot(landsat)

```

If you prefer, you could also do it the `tidyr` way:

```{r}
#| echo: true
landsat |>
  mutate(tidyr_kelvin = (LC09_L2SP_196023_20240501_20240502_02_T1_ST_B10 * 0.00341802) + 149.0) |>
  plot(., "tidyr_kelvin")
```

Now we can see that there are two columns, one of which is closer to what we want. Let’s go ahead and convert the data in Kelvins to Celsius.

```{r}
landsat$lst_celsius = landsat$lst_kelvin - 273.15
plot(landsat, "lst_celsius")
plot(hamburg, add=TRUE, border='white')
```

## **Zonal statistics**

Our original question was "What was the average surface temperature in each of Hamburg’s neighborhoods on TK date at X time?"

Now that we have our data in Celsius, let’s try to answer that.

And which of these neighborhoods is the most affected by heat?

We can actually calculate that super easily using the `extract` function.

```{r}
#| echo: true
extract(landsat$lst_celsius, hamburg, fun=mean)
```

The results show us the average temperature for each district. Annoyingly, we don’t get the name of the district, but everything aligns based on the row number. In order to save this data as a new variable in our vector file, we need to return the responses without the ID column.

```{r}
#| echo: true
hamburg$mean_lst = extract(landsat$lst_celsius, hamburg, fun=mean, ID=FALSE)

plot(hamburg, "mean_lst")
```

That was easy!

Now we can adjust some settings while plotting so that we have a continuous color scale and a better palette.

```{r}
#| echo: true
plot(hamburg, "mean_lst", type="continuous", col=hcl.colors("Reds", n=100, rev=TRUE))
```

### **Weighted means**

Something to keep in mind when working with raster data is how each pixel is handled. Our data has a resolution of 30m, which isn’t huge, but also not super detailed. That likely means there are a few pixels that land on the border between districts.

Let’s take a look at this with the neighborhood Sternschanze in Altona. (Remember, we already we filtered our data before reprojecting:

```{r}
#| echo: true

sternschanze = hamburg %>%
  filter(stadtteil_name=="Sternschanze")

landsat %>%
  crop(., sternschanze, mask=TRUE) %>%
  plot("lst_celsius")

plot(sternschanze, add=TRUE, border="white")
```

We can see that there are a lot of pixels that *touch* the border of Sternschanze, but are also partially outside.

When calling the `extract` function, it assigns the pixel by default to whichever geometry its center point falls in.

When the pixel resolution is relatively high compared to the area of geometries, then this will not have a huge effect on your results. But for smaller areas, it could skew them.

In order to understand this better, let’s first calculate the weighted mean - which only considers the approximate portion of a pixel that falls within a geometry.

```{r}
#| echo: true
hamburg$weighted_mean_lst <- extract(landsat$lst_celsius, hamburg, fun=mean, weights=TRUE, ID=FALSE)

plot(hamburg, "weighted_mean_lst", type="continuous", col=hcl.colors("Reds", n=100, rev=TRUE))
```

It doesn’t look so different from our first chart, so let’s take a look at the difference between values:

```{r}
#| echo: true
hamburg$lst_weighted_diff = hamburg$mean_lst - hamburg$weighted_mean_lst

hamburg |> ggplot(., aes(lst_weighted_diff)) +
  geom_histogram()
```

`(stat_bin()` using `bins = 30`. Pick better value with `binwidth`)

It's a pretty standard distribution and the difference in values is not so huge. That being said, neighborhoods are not all affected equally.

Let’s compare the difference in values to the area of the district. We can do that with the `expanse` function.

```{r}
#| echo: true
hamburg$area_km <- expanse(hamburg, unit="km")

hamburg |>
  ggplot(., aes(area_km, lst_weighted_diff)) +
  geom_point()
```

Now we can see that the calculations for mean surface temperature in smaller districts are much more impacted by whether or not we use the weighted mean.

So let’s use the weighted mean for our final calculation.

## **Global statistics**

Something worth noting is that we’re not necessarily interested in the temperature of each district, but how they compare to each other. What we want is the *relative* temperature. In order to do that, we need to calculate the mean temperature for all of Hamburg.

But we can’t just calculate the mean of the neighborhood means. Some neighborhoods are way larger than others, and that would distort our calculation. So, to calculate the mean value for all of hamburg, we can call the `global` function.

```{r}
#| echo: true
global(landsat, fun=mean, na.rm=TRUE) 

# without na.rm=TRUE, we'll get an error, since the pixels that have been masked out have a value of NA
```

In order to calculate relative mean temperature, we subtract the global mean from the neighborhood means (weighted, of course)

```{r}
#| echo: true
global_mean_lst <- global(landsat$lst_celsius, fun=mean, na.rm=TRUE)$mean

hamburg$weighted_relative_lst = hamburg$weighted_mean_lst - global_mean_lst

plot(hamburg, "weighted_relative_lst", type="continuous", col=hcl.colors("RdYlBu", n=100, rev=TRUE))
```

Amazing! Beautiful!

Now we can easily see which neighborhoods in Hamburg were hotter on TK date than others. We can also pull up the names.

```{r}
#| echo: true
hamburg |>
  arrange(-weighted_relative_lst) |>
  select(bezirk_name, stadtteil_name, weighted_mean_lst, weighted_relative_lst) |>
  head(10)
```

Wow, the neighborhood Billbrook was nearly 6 degrees Celsius hotter than Hamburg on average (at least, it was at the time this image was taken).

In order to look at trends over time, we need to extend our workflow a little bit.

## **Build a pipeline**

The best way to standardize a workflow is by writing some functions. So I’ve gone ahead and written a function that will do exactly what we just did with one line of code. The only input is the file path.

(If you've never written your own function in R, it's worth knowing that by wrapping a series of instructions in curly braces - {}  - and assigning a name to the function, and saving it, you can save yourself the time and trouble of either rewriting, or more likely, copying and pasting code blocks that you need to use more than once. For more on functions, see this chapter from Hadley Wickham's excellent [R for Data Science](https://r4ds.had.co.nz/index.html))

```{r}
#| echo: true
get_relative_lst = function(fpath, vector) {

  # load files
  raster = rast(fpath)

  # reproject if necessary
  if (!same.crs(raster, vector)) {
    new_crs = crs(raster)
    vector = project(vector, new_crs)
  }

  # crop and mask
  raster = crop(raster, vector, mask=TRUE)

  # convert from DN to celsius (or whatever other calculation you might need to do)
  raster = (raster * 0.00341802) + 149.0 - 273.15

  # calculate zonal statistics
  mean_lst = extract(raster, vector, fun=mean, ID=FALSE)

  # calculate global statistics
  global_mean = global(raster, fun=mean, na.rm=TRUE)$mean

  # calculate relative values
  relative_lst = mean_lst - global_mean

  # and return
  return(relative_lst)
}
```

To save some hard coding each file name, we can get a list of all the files we want in a directory. We can even pay attention to the file extension using regex!

```{r}
#| echo: true
# generate list of all images

landsat_list = list.files('data', pattern=".TIF", full.names = TRUE)

landsat_list
```

Then we can apply our function to each of the file paths in this list.

```{r}
#| echo: true
relative_lst_list <- lapply(landsat_list, get_relative_lst, vector=hamburg)
```

Since this returns a list of data frames, we can merge all the columns together to make it a bit easier to work with.

```{r}
#| echo: true
relative_lst_df = do.call(cbind, relative_lst_list)

relative_lst_df %>%
  head()
```

You’ll notice that each column is the name of the file it came from. Pretty convenient!

Now the only thing left to do is calculate the mean across each image and add the resulting values to our vector data.

```{r}
#| echo: true
hamburg$mean_relative_lst = rowMeans(relative_lst_df)

plot(hamburg, "mean_relative_lst", type="continuous", col=hcl.colors("RdYlBu", n=100, rev=TRUE))
```

## Challenge: do the same for another city

-   download a tiff file of the area you're interested in

-   adapt the relevant code blocks from this script to see the results in another part of the world

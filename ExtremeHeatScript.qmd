---
title: "Exploring Extreme Heat: Satellite imagery and spatial analysis in R"
author: "Max Donheiser"
format: html
editor: visual
---

In this tutorial, you will use Landsat satellite imagery to analyse heat islands in Hamburg (or the city of your choice).

## Getting started with `terra`

First let's load the packages we need:

```{r}
library(dplyr) # for manipulating data
library(ggplot2) # for visualization
library(terra) # for working with raster and vector data
```

Now we need some data. Using the `terra` package, we can load our raster image from Landsat and preview it.

```{r}
#| echo: true
landsat <- rast('data/LC09_L2SP_196023_20240501_20240502_02_T1_ST_B10.TIF')

plot(landsat)
```

Looks like land! But we need to find Hamburg in the image. In order to do this, let's load vector data for each of Hamburg's districts. We can do this by directly calling an API endpoint from Hamburg’s (Office for Geoinformation and Surveying)\[<https://www.hamburg.de/politik-und-verwaltung/behoerden/behoerde-fuer-stadtentwicklung-und-wohnen/aemter-und-landesbetrieb/landesbetrieb-geoinformation-und-vermessung>\] (the `terra` package accepts both files and URLs as data sources). In this example, the vector data we’re working with is a GeoJSON file, but other file formats such as Shapefiles are also accepted.

```{r}
hamburg <- vect('https://api.hamburg.de/datasets/v1/verwaltungsgrenzen/collections/stadtteile/items?limit=104&f=json')

plot(hamburg)
```

One downside to `terra` is that you can't open your data in the RStudio view like with an `sf` object. However, you can still preview your data with the standard `head` function.

```{r}
#| echo: true
hamburg |>
  head()
```

We can see that the file contains information about the district (`bezirk`) and the neighborhood (`stadtteil`).

If we want to filter the data with native `terra`, the functionality is also a bit different than with standard `dplyr`.

```{r}
#| echo: true
altona <- subset(hamburg, hamburg$bezirk_name=="Altona")

plot(altona)
```

If you wanted to stick to the classic `tidyr` and `dplyr` syntax, then you can load `tidyterra`.

```{r}
#| include: false
library(tidyterra)
```

Let's filter out the islands around Hamburg that are not part of the contiguous city.

```{r}
#| echo: true

hamburg <- hamburg |>
  filter(!(stadtteil_name %in% c("Neuwerk", "Nigehörn", "Scharhörn")))

plot(hamburg)
```

## Dealing with projections

You may have noticed that the x and y axes for our Landsat image and Hamburg vector data are quite different. That’s because the they are using two different coordinate systems. It also means that if we tried to plot both together, it wouldn't work.

We can check which CRS is being used quite easily:

```{r}
#| echo: true
crs(landsat, describe=TRUE)

crs(hamburg, describe=TRUE)
```

Here's another way to quickly check whether two geographic datasets have the same coordinate reference system.

```{r}
#| echo: true
same.crs(landsat, hamburg)
```

If we want to use both datasets together, we need to reproject one of our files. Let's reproject the data for Hamburg since it’s not as detailed as the Landsat data and will go a bit faster.

```{r}
new_crs <-  crs(landsat)

hamburg <-  hamburg |>
  project(new_crs)

plot(hamburg)
```

Notice that the plot looks the same, but the axes have different values. This is exactly what we want. Now we can plot the Hamburg districts on top of our raster data.

```{r}
plot(landsat)
plot(hamburg, add=TRUE)
```

## Cropping and masking

The Landsat image is way larger than we need. Let's crop it to just the region we're interested in.

```{r}
#| echo: true
landsat <- crop(landsat, hamburg)
plot(landsat)
plot(hamburg, add=TRUE)
```

Great, now we’re focused just on the region of Hamburg. We can also take it a step further and mask the satellite image. That means removing any areas outside of the Hamburg geometry, not just setting a bounding box.

```{r}
landsat <- mask(landsat, hamburg)
plot(landsat)
plot(hamburg, add=TRUE)
```

If you want to combine both steps, you could run the function `crop(landsat, hamburg, mask=TRUE)`.

## Raster bands and calculations

You may have noticed that the scale of our plot does not reflect the temperature in Celsius. The metric being shown is actually something called "digital numbers" or DN. I couldn't really explain to you how Landsat comes to calculate the digital number, but luckily for Landsat provides a simple [scaling formula](https://www.usgs.gov/landsat-missions/landsat-collection-2-level-2-science-products) for converting DN into Kelvins: K = (DN \* 0.00341802) + 149.0. From there, we can convert the data into Celsius.

Calculations with raster data are pretty similar to working with normal dataframes, except instead of having columns the data has different bands. Landsat GeoTIFF files only contain one band per file, let's see what it is called:

```{r}
varnames(landsat)
```

Now we can apply the scaling formula.

```{r}
landsat$lst_kelvin <-  (landsat$LC09_L2SP_196023_20240501_20240502_02_T1_ST_B10 * 0.00341802) + 149.0

plot(landsat)

```

If you prefer, you could also do it the `tidyr` way:

```{r}
#| echo: true
landsat |>
  mutate(tidyr_kelvin = (LC09_L2SP_196023_20240501_20240502_02_T1_ST_B10 * 0.00341802) + 149.0) |>
  plot("tidyr_kelvin")
```

And finally, we can convert the values from Kelvin to Celsius.

```{r}
landsat$lst_celsius = landsat$lst_kelvin - 273.15
plot(landsat, "lst_celsius")
plot(hamburg, add=TRUE, border='white')
```

## Zonal statistics

Now we move on to our analysis. How do we find which neighborhoods in Hamburg are most affected by heat? We have to calculate the mean temperature across all pixels within a neighborhood.

We can do that super easily using the `extract` function.

```{r}
#| echo: true
extract(landsat$lst_celsius, hamburg, fun=mean)
```

The results show us the average temperature for each district. Annoyingly, the ID returned with the results is just the row number of the original Hamburg data. But we can work with this. Let's save the mean surface temperature as a new variable in our hamburg data and plot the results.

```{r}
#| echo: true
hamburg$mean_lst = extract(landsat$lst_celsius, hamburg, fun=mean, ID=FALSE)

plot(hamburg, "mean_lst")
```

That was easy!

Now we can adjust some settings while plotting so that we have a continuous color scale and a better palette.

```{r}
#| echo: true
plot(hamburg, "mean_lst", type="continuous", col=hcl.colors("Reds", n=100, rev=TRUE))
```

### Weighted means

Spatial resolution is an important component when working with raster data. Our data has a resolution of 30m, which isn’t terrible, but also not super detailed. There are likely many pixels that fall on the border between two neighborhoods. Let's investigate this a bit further in the Sternschanze neighborhood of Hamburg.

```{r}
#| echo: true

sternschanze = hamburg %>%
  filter(stadtteil_name=="Sternschanze")

landsat %>%
  crop(., sternschanze, mask=TRUE) %>%
  plot("lst_celsius")

plot(sternschanze, add=TRUE, border="white")
```

As you can see, the pixels along the geographic border of the neighborhood all fall partially outside. There are a few different ways you can handle this. By default, the `extract` function assigns pixels to the geometry in which their geographic midpoint fall.When working with high-resolution raster data or large geographic regions, this doesn't pose a huge issue.

However, the smaller your regions get or the lower your resolution is, this assignment method can have large impacts on your results: If a region overlaps with 500 raster data points, then losing a few does not make a huge difference. But if it overlaps just 10 raster cells, losing just one can make a big difference.

An alternative method for evaluating zonal raster statistics is working with the weighted mean. This takes into account the approximate portion of a pixel that falls within a geometry while calculating summary statistics.

```{r}
#| echo: true
hamburg$weighted_mean_lst <- extract(landsat$lst_celsius, hamburg, fun=mean, weights=TRUE, ID=FALSE)

plot(hamburg, "weighted_mean_lst", type="continuous", col=hcl.colors("Reds", n=100, rev=TRUE))
```

This doesn't look so different to our original calculation. We can also see that in a histogram.

```{r}
#| echo: true
hamburg$lst_diff_weighted_unweighted = hamburg$mean_lst - hamburg$weighted_mean_lst

hamburg |>
  ggplot(aes(lst_diff_weighted_unweighted)) +
  geom_histogram()
```

When we look at the difference between the calculations by district size, however, there's a trend. We can do this by first calculated the district size with the `expanse` function (which is also handy!) and then plotting that against the difference between the two methods.

```{r}
#| echo: true
hamburg$area_km <- expanse(hamburg, unit="km")

hamburg |>
  ggplot(aes(area_km, lst_diff_weighted_unweighted)) +
  geom_point()
```

The difference between the weighted and unweighted mean LST varies much more for smaller districts. Let's use the weighted mean for our final calculations.

## Global statistics

For this experiment, we're not necessarily interested in the absolute surface temperature. Rather, we're interested in how they compare to each other. That's also known as the *relative* temperature. This is a pretty easy calculation: We just need to calculate the deviation from the mean temperature for all of Hamburg.

Since the neighborhoods have differing areas, the fastest way to calculate the mean temperature in Hambug is with the `global` function. This is like `extract`, but instead of zonal statistics, we get the overall - or global - value.

```{r}
#| echo: true
global(landsat, fun=mean, na.rm=TRUE) 

# without na.rm=TRUE, we'll get an error, since the pixels that have been masked out have a value of NA
```

In order to calculate relative mean temperature, we subtract the global mean from the neighborhood means (weighted, of course).

```{r}
#| echo: true
global_mean_lst <- global(landsat$lst_celsius, fun=mean, na.rm=TRUE)$mean

hamburg$weighted_relative_lst = hamburg$weighted_mean_lst - global_mean_lst

plot(hamburg, "weighted_relative_lst", type="continuous", col=hcl.colors("RdYlBu", n=100, rev=TRUE))
```

Amazing! Beautiful!

Now we can easily see which neighborhoods in Hamburg were hotter than others at the time our satellite image was taken. Let's take a closer look at the data.

```{r}
#| echo: true
hamburg |>
  arrange(-weighted_relative_lst) |>
  select(bezirk_name, stadtteil_name, weighted_mean_lst, weighted_relative_lst) |>
  head(10)
```

The Billbrook neighborhood was, on average, nearly 6 degrees Celsius hotter than Hamburg as a whole. It's a major industrial area, which means there's likely a lot of asphault and concrete and relative little green space. But we can also see that Altona-Nord	and Sternschanze, both residential areas, were also quite hot compared to the city as a whole. This is a great starting point for further reporting.

In order to look at trends over time, we need to extend our workflow a little bit.

## Bonus Section: Build a data pipeline

The best way to standardize a workflow is by writing some functions. So I’ve gone ahead and written a function that will do exactly what we just did with one line of code. The only input is the file path.

(If you've never written your own function in R, it's worth knowing that by wrapping a series of instructions in curly braces - {} - and assigning a name to the function, and saving it, you can save yourself the time and trouble of either rewriting, or more likely, copying and pasting code blocks that you need to use more than once. For more on functions, see this chapter from Hadley Wickham's excellent [R for Data Science](https://r4ds.had.co.nz/index.html))

```{r}
#| echo: true
get_relative_lst = function(fpath, vector) {

  # load files
  raster = rast(fpath)

  # reproject if necessary
  if (!same.crs(raster, vector)) {
    new_crs = crs(raster)
    vector = project(vector, new_crs)
  }

  # crop and mask
  raster = crop(raster, vector, mask=TRUE)

  # convert from DN to celsius (or whatever other calculation you might need to do)
  raster = (raster * 0.00341802) + 149.0 - 273.15

  # calculate zonal statistics
  mean_lst = extract(raster, vector, fun=mean, ID=FALSE)

  # calculate global statistics
  global_mean = global(raster, fun=mean, na.rm=TRUE)$mean

  # calculate relative values
  relative_lst = mean_lst - global_mean

  # and return
  return(relative_lst)
}
```

To save some hard coding each file name, we can get a list of all the files we want in a directory. We can even pay attention to the file extension using regex!

```{r}
#| echo: true
# generate list of all images

landsat_list = list.files('data', pattern=".TIF", full.names = TRUE)

landsat_list
```

Then we can apply our function to each of the file paths in this list.

```{r}
#| echo: true
relative_lst_list <- lapply(landsat_list, get_relative_lst, vector=hamburg)
```

Since this returns a list of data frames, we can merge all the columns together to make it a bit easier to work with.

```{r}
#| echo: true
relative_lst_df = do.call(cbind, relative_lst_list)

relative_lst_df %>%
  head()
```

You’ll notice that each column is the name of the file it came from. Pretty convenient!

Now the only thing left to do is calculate the mean across each image and add the resulting values to our vector data.

```{r}
#| echo: true
hamburg$mean_relative_lst = rowMeans(relative_lst_df)

plot(hamburg, "mean_relative_lst", type="continuous", col=hcl.colors("RdYlBu", n=100, rev=TRUE))
```

## Challenge: Do the same for another city

-   Download a tiff file of the area you're interested in
-   Download a geographic boundaries (neighborhoods, etc.)
-   Adapt the relevant code blocks from this script to see the results in another part of the world

---
title: "Exploring extreme heat: satellite imagery and spatial analysis with Landsat and R"
output:
  md_document:
    variant: markdown_github
---

```{r, include=TRUE, error=FALSE, warning=FALSE, message=FALSE}

library(dplyr) # for manipulating data
library(ggplot2) # for visualization
library(terra) # for working with raster and vector data
```

## Loading and previewing data

Conveniently, the `terra` package in R has great naming conventions for loading data. It's also really easy to plot geodata to preview how it looks. Let's start with a landsat image.

```{r}
landsat = rast('data/LC09_L2SP_196023_20240501_20240502_02_T1_ST_B10.TIF')
plot(landsat)
```

Now we can also load the vector data for Hamburg which contains the geographical boundaries for each of the city's districts. We can do this by directly calling an API endpoint from Hamburg's (Office for Geoinformation and Surveying)[https://www.hamburg.de/politik-und-verwaltung/behoerden/behoerde-fuer-stadtentwicklung-und-wohnen/aemter-und-landesbetrieb/landesbetrieb-geoinformation-und-vermessung]. In this example, the vector data we're working with is in GeoJSON format. However, the `vect` function in `terra` can also handle other vector data formats, such as Shapefiles.

```{r}
hamburg = vect('https://api.hamburg.de/datasets/v1/verwaltungsgrenzen/collections/stadtteile/items?limit=104&f=json')
plot(hamburg)
```

One of the drawbacks of `terra` is that you can't click on and explore
your dataset from the R environment the same way you could with a normal
data frame or the `sf` package. That being said, you can preview your
data using the standard `head` function, which gives a nice overview of
what data you're working with.

```{r}
hamburg %>%
  head()
```

In our case, we have information on the city district (`bezirk`) and the
neighborhood (`stadtteil`). We can also filter the data, although it
functions a bit differently than with normal dataframes.

Let's try plot just the neighborhoods in Hamburg's Altona district.

```{r}
altona = subset(hamburg, hamburg$bezirk_name=="Altona")
plot(altona)
```

If you want to stick to the classic `tidyr` and `dplyr` syntax, then you
can use the helpful `tidyterra` package. It allows us to interact with
spatial raster or vector data the same way we would with a dataframe.

```{r}
library(tidyterra)

# let's filter out the islands of hamburg so we can only focus on the city
hamburg = hamburg %>%
  filter(!(stadtteil_name %in% c("Neuwerk", "Nigehörn", "Scharhörn")))

plot(hamburg)
```

## Dealing with projections

You may have noticed that the x and y axes for our Landsat image and
Hamburg are quite different. That's because the they are using two
different coordinate systems. It also means that if we tried to plot
both together, it wouldn't really work out.

In technical terms, that means the data sets have different coordinate
reference systems, also known as CRS. We can check which CRS is being
used quite easily:

```{r}
crs(landsat)
crs(hamburg)
```

That returns a bunch of information, not all of which we need. An easier
alternative?

```{r}
same.crs(landsat, hamburg)
```

Great, a simple no. In order to fix this, we need to reproject either
Hamburg or the Landsat data. In this case, I'll reproject the data for
Hamburg since it's not as detailed as the Landsat data and will go a bit
faster.

```{r}
new_crs = crs(landsat)

hamburg = hamburg %>%
  project(new_crs)

plot(hamburg)
```

Wonderful, this seems to be a bit closer to what we want. To double
check that everything worked, we can plot the Hamburg districts on top
of the raster data.

```{r}
plot(landsat)
plot(hamburg, add=TRUE)
```

## Cropping and masking

Now, you may have noticed that the Landsat image is, like, way bigger
than we need. We can take two measures to reduce the amount of data
we're working with, and consequently speed up calculations that we'll do
later. The first is cropping the image. This is the same thing as
cropping out that random stranger who happened to be in your holiday
vacation photos.

```{r}
landsat = crop(landsat, hamburg)
plot(landsat)
plot(hamburg, add=TRUE)
```

Great, now we're focused just on the region of Hamburg. But we can also
take it a step further and mask the Landsat image. This basically means
removing parts of the image that are not within our zone of interest.
Think about a PNG file that has a transparent background in parts of it.

```{r}
landsat = mask(landsat, hamburg)
plot(landsat)
plot(hamburg, add=TRUE)
```

If you want to speed things up, you can do this in one step by calling
the `crop` function and setting the variable `mask` to true.

## Bands and calculations with raster data

Another thing you may have noticed is that the scale for the Landsat
image doesn't make sense. We're working with surface temperature, and
those numbers surely don't represent anything close to Celsius or
Fahrenheit. That's because the data is natively in a metric called
digital numbers (DN). Unfortunately, I can't really tell you what this
means. Fortunately, Landsat provides us with a nice little [scaling
formula](https://www.usgs.gov/landsat-missions/landsat-collection-2-level-2-science-products)
to turn these values into Kelvins: K = (DN \* 0.00341802) + 149.0

The way calculations in raster data work is that, much like mutating an
entire column, each pixel will be put through the given formula. Instead
of columns, however, we have something called bands. Let's see what
different bands we have.

```{r}
varnames(landsat)
```

Alright, let's make a new band for the same data in Kelvins.

```{r}
landsat$lst_kelvin = (landsat$LC09_L2SP_196023_20240501_20240502_02_T1_ST_B10 * 0.00341802) + 149.0
plot(landsat)
```

If you want, you could also do it the `tidyr` way.

```{r}
landsat %>%
  mutate(tidyr_kelvin = (LC09_L2SP_196023_20240501_20240502_02_T1_ST_B10 * 0.00341802) + 149.0) %>%
  plot(., "tidyr_kelvin")
```

Now we can see that there are two columns, one of which is closer to
what we want. Let's go ahead and convert the data in Kelvins to Celsius.

```{r}
landsat$lst_celsius = landsat$lst_kelvin - 273.15
plot(landsat, "lst_celsius")
plot(hamburg, add=TRUE, border='white')
```

## Zonal statistics

### Means

Now that we have our data in Celsius, let's try to answer our original
question: What was the average surface temperature in each of Hamburg's
neighborhoods on TK date at X time? And which of these neighborhoods in
the most affected by head? We can actually calculate that super easily
using the `extract` function.

```{r}
extract(landsat$lst_celsius, hamburg, fun=mean)
```

The results show us the average temperature for each district.
Annoyingly, we don't get the name of the district, but everything aligns
based on the row number. In order to save this data as a new variable in
our vector file, we need to return the responses without the ID column.

```{r}
hamburg$mean_lst = extract(landsat$lst_celsius, hamburg, fun=mean, ID=FALSE)
plot(hamburg, "mean_lst")
```

That was easy! We can also adjust some settings while plotting so that
we have a continuous color scale and a better palette.

```{r}
plot(hamburg, "mean_lst", type="continuous", col=hcl.colors("Reds", n=100, rev=TRUE))
```

### Weighted means

Something to keep in mind when working with raster data is how each
pixel is handled. Our data has a resolution of 30m, which isn't huge,
but also not super detailed. That likely means there are a few pixels
that land on the border between districts. Let's take a look at this
with the neighborhood Sternschanze in Altona.

```{r}
# since we filtered data before reprojecting
sternschanze = hamburg %>%
  filter(stadtteil_name=="Sternschanze")

landsat %>%
  crop(., sternschanze, mask=TRUE) %>%
  plot("lst_celsius")

plot(sternschanze, add=TRUE, border="white")
```

We can see that there are a lot of pixels that *touch* the border of
Sternschanze, but are also partially outside. When calling the `extract`
function, the default functionality is to assign the pixel to whichever
geometry its center point falls in. When the pixel resolution is
relatively high compared to the area of geometries, then this will not
have a huge effect on your results. But for smaller areas, it could skew
them.

In order to understand this better, let's first calculate the weighted
mean - which only considers the approximate portion of a pixel that
falls within a geometry.

```{r}
hamburg$weighted_mean_lst = extract(landsat$lst_celsius, hamburg, fun=mean, weights=TRUE, ID=FALSE)
plot(hamburg, "weighted_mean_lst", type="continuous", col=hcl.colors("Reds", n=100, rev=TRUE))
```

It doesn't look so different from our first chart, so let's take a look
at the difference between values.

```{r}
hamburg$lst_weighted_diff = hamburg$mean_lst - hamburg$weighted_mean_lst

hamburg %>%
  ggplot(., aes(lst_weighted_diff)) +
  geom_histogram()
```

Pretty standard distribution and the difference in values is not so
huge. That being said, not each neighborhood is affected equally. Let's
compare the difference in values to the area of the distrct. We can use
that with the `expanse` function.

```{r}
hamburg$area_km = expanse(hamburg, unit="km")

hamburg %>%
  ggplot(., aes(area_km, lst_weighted_diff)) +
  geom_point()
```

Now it's very easy to see that the calculations for mean surface
temperature in smaller districts are much more impacted by whether or
not we use the weighted mean. So let's use the weighted mean for our
final calculation.

## Global statistics

Something worth noting is that we're not necessarily interested in the temperature of each district, but how they compare to each other. What we want is the *relative* temperature. In order to do that, we need to calculate the mean temperature for all of Hamburg.

But we can't just calculate the mean of the neighborhood means. Some neighborhoods are way larger than others, which would disproportionately affect our calculation. So, to calculate the mean value for all of hamburg, we can call the `global` function.

```{r}
global(landsat, fun=mean, na.rm=TRUE) # without na.rm=TRUE, we'll get an error, since the pixels that have been masked out have a value of NA
```

In order to calculate relative mean temperature, we then just subtract the global mean from the neighborhood means (weighted, of course).

```{r}
global_mean_lst = global(landsat$lst_celsius, fun=mean, na.rm=TRUE)$mean

hamburg$weighted_relative_lst = hamburg$weighted_mean_lst - global_mean_lst

plot(hamburg, "weighted_relative_lst", type="continuous", col=hcl.colors("RdYlBu", n=100, rev=TRUE))
```

Amazing! Beautiful! Now we can easily see which neighborhoods in Hamburg were hotter on TK date than others. We can also pull up the names.

```{r}
hamburg %>%
  arrange(-weighted_relative_lst) %>%
  select(bezirk_name, stadtteil_name, weighted_mean_lst, weighted_relative_lst) %>%
  head(10)
```

Wow, the neighborhood Billbrook was nearly 6 degrees Celsius hotter than Hamburg on average. At least, at the time this image was taken. In order to look at trends over time, we need to extend our workflow a little bit.

## Build a pipeline

The best way to standardize a workflow is by writing some functions. So I've gone ahead and written a function that will do exactly what we just did with one line of code. The only input is the file path.

```{r}
get_relative_lst = function(fpath, vector) {

  # load files
  raster = rast(fpath)

  # reproject if necessary
  if (!same.crs(raster, vector)) {
    new_crs = crs(raster)
    vector = project(vector, new_crs)
  }

  # crop and mask
  raster = crop(raster, vector, mask=TRUE)

  # convert from DN to celsius (or whatever other calculation you might need to do)
  raster = (raster * 0.00341802) + 149.0 - 273.15

  # calculate zonal statistics
  mean_lst = extract(raster, vector, fun=mean, ID=FALSE)

  # calculate global statistics
  global_mean = global(raster, fun=mean, na.rm=TRUE)$mean

  # calculate relative values
  relative_lst = mean_lst - global_mean

  # and return
  return(relative_lst)
}
```

To save some hard coding each file name, we can get a list of all the files we want in a directory. We can even pay attention to the file extension using regex!

```{r}
# generate list of all images

landsat_list = list.files('data', pattern=".TIF", full.names = TRUE)
landsat_list
```

Then we can apply our function to each of the file paths in this list.

```{r}
relative_lst_list = lapply(landsat_list, get_relative_lst, vector=hamburg)
```

Since this returns a list of data frames, we can merge all the columns together to make it a bit easier to work with.

```{r}
relative_lst_df = do.call(cbind, relative_lst_list)

relative_lst_df %>%
  head()
```

You'll notice that each column is the name of the file where it came from. Pretty convenient! Now the only thing left to do is calculate the mean across each image and add the resulting values to our vector data.

```{r}
hamburg$mean_relative_lst = rowMeans(relative_lst_df)

plot(hamburg, "mean_relative_lst", type="continuous", col=hcl.colors("RdYlBu", n=100, rev=TRUE))
```
